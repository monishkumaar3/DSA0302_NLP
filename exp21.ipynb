{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNU+deexlQpsNRVOX+UQoDS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-SIVDOnz3kiM","executionInfo":{"status":"ok","timestamp":1700620576176,"user_tz":-330,"elapsed":6172,"user":{"displayName":"Monish Kumaar","userId":"15042714021371901584"}},"outputId":"a2f3a152-7be2-4aa8-a54d-8a194bde4892"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"]}],"source":["pip install nltk\n"]},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Sample sentences\n","sentences = [\n","    \"Artificial intelligence (AI) is a field of computer science.\",\n","    \"Machine learning is a subset of AI that focuses on training models to make predictions.\",\n","    \"Deep learning is a type of machine learning that uses neural networks with multiple layers.\",\n","    \"Neural networks are composed of interconnected nodes called neurons.\",\n","    \"Recurrent neural networks (RNNs) are commonly used in natural language processing tasks.\"\n","]\n","\n","# Tokenization and preprocessing\n","def preprocess(sentence):\n","    tokens = word_tokenize(sentence.lower())  # Tokenization and convert to lowercase\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]  # Remove stopwords and non-alphanumeric tokens\n","    stemmer = PorterStemmer()\n","    tokens = [stemmer.stem(token) for token in tokens]  # Stemming\n","    return ' '.join(tokens)\n","\n","preprocessed_sentences = [preprocess(sentence) for sentence in sentences]\n","\n","# TF-IDF vectorization\n","vectorizer = TfidfVectorizer()\n","tfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n","\n","# Get feature names (words)\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Extract top keywords or phrases based on TF-IDF scores\n","def get_top_keywords(doc, top_n=5):\n","    feature_index = tfidf_matrix[doc].nonzero()[1]\n","    tfidf_scores = zip(feature_index, [tfidf_matrix[doc, x] for x in feature_index])\n","    sorted_scores = sorted(tfidf_scores, key=lambda x: (x[1]), reverse=True)\n","    top_features = [feature_names[i] for i, _ in sorted_scores[:top_n]]\n","    return top_features\n","\n","# Extract top keywords/phrases for each sentence\n","for i, sentence in enumerate(sentences):\n","    print(f\"Keywords for Sentence {i+1}: {get_top_keywords(i)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"trrD9xwf4scw","executionInfo":{"status":"ok","timestamp":1700620714431,"user_tz":-330,"elapsed":369,"user":{"displayName":"Monish Kumaar","userId":"15042714021371901584"}},"outputId":"df6df6d7-3a72-413f-9eeb-659e6cb3cddf"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Keywords for Sentence 1: ['scienc', 'comput', 'field', 'intellig', 'artifici']\n","Keywords for Sentence 2: ['predict', 'make', 'model', 'train', 'focus']\n","Keywords for Sentence 3: ['learn', 'layer', 'multipl', 'type', 'deep']\n","Keywords for Sentence 4: ['neuron', 'call', 'node', 'interconnect', 'compos']\n","Keywords for Sentence 5: ['task', 'process', 'languag', 'natur', 'commonli']\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]}]}